{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet langchain sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hys0j7XfFVoc",
        "outputId": "0850d90a-15ef-4f75-9762-6d4c7e934b6f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/412.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG5LsqcNODDd",
        "outputId": "af6fe91f-62fc-47a2-ae91-fd5757bd5c48"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHMYWwngFq2c",
        "outputId": "9b44b003-f465-4fe4-ec89-bd14167e0acd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.27.1)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.3.31)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (3.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.21.0)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.2.10)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.10.5)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (9.0.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (11.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain_huggingface) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain_huggingface) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain_huggingface) (0.5.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.10.14)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.12.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.3.1)\n",
            "Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: langchain_huggingface\n",
            "Successfully installed langchain_huggingface-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x-B3ypEvM2vC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "model = SentenceTransformer('thenlper/gte-large-zh')"
      ],
      "metadata": {
        "id": "P1kzBRSbFY3p"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import top_k_accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sentence_transformers import util\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "Qo36Ili_ZTBc"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "文本嵌入"
      ],
      "metadata": {
        "id": "avyaUz_qMvjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算QQP任务指标\n",
        "def evaluate_qqp(model, qqp_data):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    # 遍历所有数据项\n",
        "    for idx in range(len(qqp_data['question1'])):\n",
        "        question1 = qqp_data['question1'][idx]\n",
        "        question2 = qqp_data['question2'][idx]\n",
        "        label = qqp_data['label'][idx]\n",
        "\n",
        "        # 获取嵌入\n",
        "        embeddings_q1 = model.encode([question1])\n",
        "        embeddings_q2 = model.encode([question2])\n",
        "\n",
        "        # 计算余弦相似度\n",
        "        cosine_sim = cosine_similarity(embeddings_q1, embeddings_q2)[0][0]\n",
        "\n",
        "        # 判断相似性，余弦相似度 > 0.9 认为是相似\n",
        "        prediction = 1 if cosine_sim > 0.9 else 0\n",
        "\n",
        "        # 记录实际标签和预测标签\n",
        "        y_true.append(label)\n",
        "        y_pred.append(prediction)\n",
        "\n",
        "    # 计算准确率、召回率、精确率\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "\n",
        "    return acc, precision, recall"
      ],
      "metadata": {
        "id": "FrQSSGfyOaw1"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算ANA任务的前1、前3准确率\n",
        "def evaluate_analogy(model, analogy_data):\n",
        "    correct_1 = 0\n",
        "    correct_2 = 0\n",
        "    correct_3 = 0\n",
        "\n",
        "    # 遍历所有数据项\n",
        "    for idx in range(len(analogy_data['stem'])):\n",
        "        stem = analogy_data['stem'][idx]\n",
        "        answer = analogy_data['answer'][idx]\n",
        "        choices = analogy_data['choice'][idx]\n",
        "\n",
        "        # 获取嵌入\n",
        "        embeddings_stem_1 = model.encode(stem[0])  # stem 第一个词的嵌入\n",
        "        embeddings_stem_2 = model.encode(stem[1])  # stem 第二个词的嵌入\n",
        "\n",
        "        # 计算stem两个词汇之间的相似度\n",
        "        cosine_sim_stem = cosine_similarity([embeddings_stem_1], [embeddings_stem_2])[0][0]\n",
        "\n",
        "        # 计算每个选项的相似度差值\n",
        "        cosine_differences = []\n",
        "        for choice in choices:\n",
        "            embeddings_choice_1 = model.encode(choice[0])  # 选项第一个词的嵌入\n",
        "            embeddings_choice_2 = model.encode(choice[1])  # 选项第二个词的嵌入\n",
        "\n",
        "            # 计算当前选项两个词的相似度\n",
        "            cosine_sim_choice = cosine_similarity([embeddings_choice_1], [embeddings_choice_2])[0][0]\n",
        "\n",
        "            # 计算与stem的相似度差值\n",
        "            cosine_diff = abs(cosine_sim_stem - cosine_sim_choice)\n",
        "            cosine_differences.append(cosine_diff)\n",
        "\n",
        "        # 获取按相似度差值排序后的索引\n",
        "        top_indices = np.argsort(cosine_differences)\n",
        "\n",
        "        # 判断前1、2、3、5个选项是否正确\n",
        "        if top_indices[0] == answer:\n",
        "            correct_1 += 1\n",
        "        if top_indices[1] == answer:\n",
        "            correct_2 += 1\n",
        "        if top_indices[2] == answer:\n",
        "            correct_3 += 1\n",
        "\n",
        "    # 计算前1、2、3、5的准确率\n",
        "    acc_1 = correct_1 / len(analogy_data['stem'])\n",
        "    acc_2 = correct_2 / len(analogy_data['stem'])\n",
        "    acc_3 = correct_3 / len(analogy_data['stem'])\n",
        "    # acc_5 = correct_5 / len(analogy_data['stem'])\n",
        "\n",
        "    return acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "id": "u5U1MfHkOhNH"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_load():\n",
        "  analogy_dataset = load_dataset(\"relbert/analogy_questions\",\"bats\")\n",
        "  print(analogy_dataset['test'][:2])\n",
        "  qqp_dataset = load_dataset(\"glue\", \"qqp\")\n",
        "  print(qqp_dataset['train'][:10])\n",
        "  return analogy_dataset['test'][:300],qqp_dataset['train'][:300]"
      ],
      "metadata": {
        "id": "zEDh7SVeOlKq"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ana,qqp=data_load()\n",
        "\n",
        "acc_1, acc_2, acc_3=evaluate_analogy(model,ana)\n",
        "acc, precision, recall=evaluate_qqp(model,qqp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f0tmlZuXa9uS",
        "outputId": "32817124-cf1b-4b2a-8b38-f3957bf68a75"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'stem': [['hitler', 'dictator'], ['rousseau', 'writer']], 'answer': [0, 3], 'choice': [[['strauss', 'composer'], ['kepler', 'hegel'], ['wagner', 'beethoven'], ['ecuador', 'spanish']], [['cattle', 'calf'], ['edison', 'hawking'], ['rembrandt', 'picasso'], ['hegel', 'philosopher']]], 'prefix': ['./cache/BATS_3.0/3_Encyclopedic_semantics/E05 [name - occupation].txt', './cache/BATS_3.0/3_Encyclopedic_semantics/E05 [name - occupation].txt']}\n",
            "{'question1': ['How is the life of a math student? Could you describe your own experiences?', 'How do I control my horny emotions?', 'What causes stool color to change to yellow?', 'What can one do after MBBS?', 'Where can I find a power outlet for my laptop at Melbourne Airport?', \"How not to feel guilty since I am Muslim and I'm conscious we won't have sex together?\", 'How is air traffic controlled?', 'What is the best self help book you have read? Why? How did it change your life?', \"Can I enter University of Melbourne if I couldn't achieve the guaranteed marks in Trinity College Foundation?\", 'Do you need a passport to go to Jamaica from the United States?'], 'question2': ['Which level of prepration is enough for the exam jlpt5?', 'How do you control your horniness?', 'What can cause stool to come out as little balls?', 'What do i do after my MBBS ?', 'Would a second airport in Sydney, Australia be needed if a high-speed rail link was created between Melbourne and Sydney?', \"I don't beleive I am bulimic, but I force throw up atleast once a day after I eat something and feel guilty. Should I tell somebody, and if so who?\", 'How do you become an air traffic controller?', 'What are the top self help books I should read?', 'University of the Philippines: If I take a second BFA in the UP College of Fine Arts, can I be exempted from gen. ed. or core subjects?', 'How can I move to Jamaica?'], 'label': [0, 1, 0, 1, 0, 0, 0, 1, 0, 0], 'idx': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Analogy Task - Top 1 Accuracy: {acc_1*100:.2f}%\")\n",
        "print(f\"Analogy Task - Top 2 Accuracy: {(acc_2+acc_1)*100:.2f}%\")\n",
        "print(f\"Analogy Task - Top 3 Accuracy: {(acc_2+acc_1+acc_3)*100:.2f}%\")\n",
        "\n",
        "# 打印 qqp 任务的准确率、精确度和召回率\n",
        "print(f\"QQP Task - Accuracy: {acc*100:.2f}%\")\n",
        "print(f\"QQP Task - Precision: {precision*100:.2f}%\")\n",
        "print(f\"QQP Task - Recall: {recall*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnnohVPbpY6w",
        "outputId": "4dd34f50-ec0e-44cb-ae29-a5a53e3a9794"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analogy Task - Top 1 Accuracy: 40.00%\n",
            "Analogy Task - Top 2 Accuracy: 66.33%\n",
            "Analogy Task - Top 3 Accuracy: 80.33%\n",
            "QQP Task - Accuracy: 66.33%\n",
            "QQP Task - Precision: 83.33%\n",
            "QQP Task - Recall: 9.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NsenA-pKpXnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import words\n",
        "\n",
        "# 下载词汇列表\n",
        "nltk.download('words')\n",
        "\n",
        "# 获取英文单词列表\n",
        "word_list = words.words()\n",
        "print(word_list[:10])  # 打印前10个单词"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "H1OwJY35hUKH",
        "outputId": "8e3ccb87-69e9-4cce-bff1-98e41799183b"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', 'Aaron']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    }
  ]
}